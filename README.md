# Optimization Methods in Deep Learning 

This assignment applies advanced optimization algorithms such as **Mini-batch Gradient Descent**, **Momentum**, and **Adam** to train deep neural networks more efficiently and reliably.  
It is part of **Week 2 (Course 2: Improving Deep Neural Networks – Hyperparameter Tuning, Regularization, and Optimization)** from the **Deep Learning Specialization** by **Andrew Ng** on Coursera.

##  Description

In this lab, I implemented and compared different optimization techniques for training deep neural networks. These methods help speed up convergence, reduce oscillations during training, and improve overall model performance.

### Key Concepts Covered:
- Mini-batch Gradient Descent
- Exponentially Weighted Averages (Momentum)
- Adam Optimizer (Adaptive Moment Estimation)
- Hyperparameter tuning for learning rates and batch sizes
- Analyzing optimization curves and convergence behavior
- Understanding the impact of optimizers on training stability

##  Files Included

- `optimization_methods_lab.ipynb`: Main notebook containing the implementation of different optimizers
- `optimization_utils.py`: Utility functions for initializing parameters, forward/backward propagation, and training loops
- `data/`: Dataset used for training and testing models under different optimizers

> ⚠️ This repository contains only my original implementation and complies with Coursera’s Honor Code.

##  Tools Used

- Python 3
- NumPy
- Matplotlib
- Jupyter Notebook

##  Course Info

This lab is part of:
> [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)  
> Instructor: **Andrew Ng**  
> Course 2: Improving Deep Neural Networks  
> Week 2: Optimization Algorithms

##  License

This repository is intended for educational and portfolio use only. Not for direct submission on Coursera.

---

 If you find optimization methods fascinating, feel free to star this repository!
